{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "import paralleldots\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Masking, Dense, concatenate, multiply, subtract, Dropout, Embedding, LSTM, GRU, Bidirectional, GlobalMaxPooling1D, Input, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from my_layers import SelfAttLayer, weightedAccCallback\n",
    "from score import score_submission, print_confusion_matrix, report_score\n",
    "from keras.preprocessing import sequence\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import unidecode\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import pickle\n",
    "import unicodedata\n",
    "#import gensim\n",
    "from nltk import tokenize\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from feature_engineering import polarity_features, refuting_features, word_overlap_features, hand_features\n",
    "#from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from rouge import Rouge\n",
    "import jellyfish\n",
    "import string\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import unidecode\n",
    "rouge = Rouge()\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_list = [\"n't\",\"'d\",\"'ll\",\"'s\",\"'m\",\"'ve\",\"'re\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def closest_word(originalWord, embeddings):\n",
    "    words = list(embeddings.keys())\n",
    "    currentClosest = words[0]\n",
    "    for word in words:\n",
    "        if jellyfish.jaro_winkler(originalWord, word) > jellyfish.jaro_winkler(originalWord, currentClosest):\n",
    "            currentClosest = word\n",
    "    print(\"Closest word to \" + originalWord +\" is \" + currentClosest)\n",
    "    return embeddings[currentClosest]\n",
    "\n",
    "def remove_parenthesis(sent):\n",
    "    return ' '.join(sent.replace('(', ' ').replace(')', ' ').replace('.', '').split()).lower()\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "def clean_fnc(s):\n",
    "    s = unidecode.unidecode(s) # for correct tokenization\n",
    "    tokens = word_tokenize(s)\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok not in abbr_list:\n",
    "            tokens[i] = clean(tok)\n",
    "    return ' '.join(list(filter(lambda x: x != '', tokens))).lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening features\n",
      "Opening variables\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 300\n",
    "max_seq_len = 50\n",
    "max_seqs = 30\n",
    "print(\"Opening features\")\n",
    "with open('features.pkl', 'rb') as inpFeat:\n",
    "    overlapFeatures_fnc = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc = pickle.load(inpFeat)\n",
    "    handFeatures_fnc = pickle.load(inpFeat)\n",
    "    overlapFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    handFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_nli = pickle.load(inpFeat)\n",
    "    refutingFeatures_nli = pickle.load(inpFeat)\n",
    "    polarityFeatures_nli = pickle.load(inpFeat)\n",
    "    handFeatures_nli = pickle.load(inpFeat)\n",
    "    overlapFeatures_nli_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_nli_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_nli_test = pickle.load(inpFeat)\n",
    "    handFeatures_nli_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_matched_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_matched_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_matched_test = pickle.load(inpFeat)\n",
    "    handFeatures_matched_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    handFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    handFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    overlapFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    handFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    bleu_nli = pickle.load(inpFeat)\n",
    "    bleu_nli_test = pickle.load(inpFeat)\n",
    "    bleu_matched = pickle.load(inpFeat)\n",
    "    bleu_mismatched = pickle.load(inpFeat)\n",
    "    rouge_nli = pickle.load(inpFeat)\n",
    "    rouge_nli_test = pickle.load(inpFeat)\n",
    "    rouge_matched = pickle.load(inpFeat)\n",
    "    rouge_mismatched = pickle.load(inpFeat)\n",
    "    bleu_fnc = pickle.load(inpFeat)\n",
    "    bleu_fnc_test = pickle.load(inpFeat)\n",
    "    bleu_two_sentences = pickle.load(inpFeat)\n",
    "    bleu_two_sentences_test = pickle.load(inpFeat)\n",
    "    rouge_fnc = pickle.load(inpFeat)\n",
    "    rouge_fnc_test = pickle.load(inpFeat)\n",
    "    rouge_two_sentences = pickle.load(inpFeat)\n",
    "    rouge_two_sentences_test = pickle.load(inpFeat)\n",
    "\n",
    "del overlapFeatures_nli, refutingFeatures_nli, polarityFeatures_nli, handFeatures_nli, overlapFeatures_nli_test , refutingFeatures_nli_test, \\\n",
    "    polarityFeatures_nli_test, handFeatures_nli_test, overlapFeatures_matched_test, refutingFeatures_matched_test, polarityFeatures_matched_test, \\\n",
    "    handFeatures_matched_test, overlapFeatures_mismatched_test, refutingFeatures_mismatched_test, polarityFeatures_mismatched_test, \\\n",
    "    handFeatures_mismatched_test, bleu_nli, bleu_nli_test, bleu_matched, bleu_mismatched, rouge_nli, rouge_nli_test, rouge_matched, rouge_mismatched\n",
    "\n",
    "print(\"Opening variables\")\n",
    "with open('variables.pkl', 'rb') as inp:\n",
    "    embedding_weights = pickle.load(inp)\n",
    "    X1 = pickle.load(inp)\n",
    "    X2 = pickle.load(inp)\n",
    "    Y = pickle.load(inp)\n",
    "    X1_test_old = pickle.load(inp)\n",
    "    X2_test_old = pickle.load(inp)\n",
    "    Y_test = pickle.load(inp)\n",
    "    X1_nli = pickle.load(inp)\n",
    "    X2_nli = pickle.load(inp)\n",
    "    Y_nli = pickle.load(inp)\n",
    "    X1_test_nli = pickle.load(inp)\n",
    "    X2_test_nli = pickle.load(inp)\n",
    "    Y_test_nli = pickle.load(inp)\n",
    "    X1_test_matched = pickle.load(inp)\n",
    "    X2_test_matched = pickle.load(inp)\n",
    "    Y_test_matched = pickle.load(inp)\n",
    "    X1_test_mismatched = pickle.load(inp)\n",
    "    X2_test_mismatched = pickle.load(inp)\n",
    "    Y_test_mismatched = pickle.load(inp)\n",
    "    X2_two_sentences = pickle.load(inp)\n",
    "    X2_test_two_sentences = pickle.load(inp)\n",
    "    tokenizer = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = ['Trump gets cheers to luck hum up at the world series  ']\n",
    "X2_test = ['President Donald Trump received a mix of cheers and boos as he was shown to the crowd at the World Series.The President and First Lady Melania Trump sat in a suite behind home plate at Nationals Park, joined by some Republican members of Congress including Rep. Steve Scalise, Sen. Lindsay Graham, and Rep. Matt Gaetz, when they were shown on the videoboard in right field during a salute to veterans.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(  X1_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = sequence.pad_sequences( tokenizer.texts_to_sequences( X1_test ) , maxlen=max_seq_len ) # Mesma coisa para headlines de teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = np.asarray( X1_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aux = np.zeros( ( len(X2_test) , max_seqs , max_seq_len ) ) # len(X2) = numero total de bodies do dataset, max_seq_len = 30, max_seqs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentences in enumerate(X2_test):\n",
    "    sentences = sent_tokenize( sentences )\n",
    "    sentences = list(map(lambda x: '| ' + clean_fnc(x) + ' |', sentences))\n",
    "    aux = [ ]\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_seqs: data_aux[i,j] = sequence.pad_sequences( tokenizer.texts_to_sequences( [ sent ] ) , maxlen=max_seq_len )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test = np.asarray( data_aux )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening similarities\n",
      "Done\n",
      "Done ! Done ! Done !\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 30, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "HeadlineEncoder (Model)         (None, 600)          61442400    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "DocumentEncoder (Model)         (None, 600)          63604800    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1200)         0           HeadlineEncoder[1][0]            \n",
      "                                                                 DocumentEncoder[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 600)          0           HeadlineEncoder[1][0]            \n",
      "                                                                 DocumentEncoder[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 600)          0           HeadlineEncoder[1][0]            \n",
      "                                                                 DocumentEncoder[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1200)         0           HeadlineEncoder[1][0]            \n",
      "                                                                 HeadlineEncoder[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 600)          0           HeadlineEncoder[1][0]            \n",
      "                                                                 HeadlineEncoder[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 600)          0           HeadlineEncoder[1][0]            \n",
      "                                                                 HeadlineEncoder[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 2450)         0           concatenate_5[0][0]              \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 2450)         0           concatenate_3[0][0]              \n",
      "                                                                 multiply_1[0][0]                 \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "                                                                 input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2450)         0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2450)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 600)          1470600     dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 600)          1470600     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 600)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 600)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 300)          180300      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 300)          180300      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 300)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 300)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 41)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1352)         0           dropout_8[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "                                                                 input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "                                                                 input_25[0][0]                   \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1352)         0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            5412        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 66,912,012\n",
      "Trainable params: 6,912,012\n",
      "Non-trainable params: 60,000,000\n",
      "__________________________________________________________________________________________________\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "del X1_nli, X2_nli, Y_nli, X1_test_nli, X2_test_nli, Y_test_nli, X1_test_matched, X2_test_matched, Y_test_matched, X1_test_mismatched, \\\n",
    "    X2_test_mismatched, Y_test_mismatched\n",
    "\n",
    "print(\"Opening similarities\")\n",
    "with open('SOLATFeatures/similarity.pkl', 'rb') as inpSim:\n",
    "    cosFeatures = pickle.load(inpSim)\n",
    "    cosFeatures_test = pickle.load(inpSim)\n",
    "    cosFeatures_nli = pickle.load(inpSim)\n",
    "    cosFeatures_nli_test = pickle.load(inpSim)\n",
    "    cosFeatures_matched = pickle.load(inpSim)\n",
    "    cosFeatures_mismatched = pickle.load(inpSim)\n",
    "\n",
    "cosFeatures = np.array(cosFeatures)\n",
    "cosFeatures_test = np.array(cosFeatures_test)\n",
    "\n",
    "cosFeatures_fnc = []\n",
    "cosFeatures_two = []\n",
    "for feat in cosFeatures:\n",
    "    cosFeatures_fnc += [feat[0]]\n",
    "    cosFeatures_two += [feat[1]]\n",
    "cosFeatures_fnc = np.array(cosFeatures_fnc)\n",
    "cosFeatures_two = np.array(cosFeatures_two)\n",
    "\n",
    "cosFeatures_fnc_test = []\n",
    "cosFeatures_two_test = []\n",
    "for feat in cosFeatures_test:\n",
    "    cosFeatures_fnc_test += [feat[0]]\n",
    "    cosFeatures_two_test += [feat[1]]\n",
    "cosFeatures_fnc_test = np.array(cosFeatures_fnc_test)\n",
    "cosFeatures_two_test = np.array(cosFeatures_two_test)\n",
    "\n",
    "del cosFeatures_nli, cosFeatures_nli_test, cosFeatures_matched, cosFeatures_mismatched\n",
    "\n",
    "with open(\"SOLATFeatures/cider_fnc.pkl\", \"rb\") as ciderFile:\n",
    "    cider_fnc_train = pickle.load(ciderFile, encoding='latin1')\n",
    "    cider_fnc_test = pickle.load(ciderFile, encoding='latin1')\n",
    "    cider_two_train = pickle.load(ciderFile, encoding='latin1')\n",
    "    cider_two_test = pickle.load(ciderFile, encoding='latin1')\n",
    "\n",
    "import pickle as cPickle\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.basic.pkl\", \"rb\") as countsTrain:\n",
    "    names = cPickle.load(countsTrain)\n",
    "    talos_counts_train = cPickle.load(countsTrain, encoding='latin1')\n",
    "print('Done')\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.basic.pkl\", \"rb\") as countsTest:\n",
    "    names = cPickle.load(countsTest)\n",
    "    talos_counts_test = cPickle.load(countsTest, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.sim.tfidf.pkl\", \"rb\") as tfidfSim_train:\n",
    "    talos_tfidfsim_train = cPickle.load(tfidfSim_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.sim.tfidf.pkl\", \"rb\") as tfidfSim_test:\n",
    "    talos_tfidfsim_test = cPickle.load(tfidfSim_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.headline.svd.pkl\", \"rb\") as svdHealine_train:\n",
    "    talos_svdHeadline_train = cPickle.load(svdHealine_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.headline.svd.pkl\", \"rb\") as svdHealine_test:\n",
    "    talos_svdHeadline_test = cPickle.load(svdHealine_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.body.svd.pkl\", \"rb\") as svdBody_train:\n",
    "    talos_svdBody_train = cPickle.load(svdBody_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.body.svd.pkl\", \"rb\") as svdBody_test:\n",
    "    talos_svdBody_test = cPickle.load(svdBody_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.sim.svd.pkl\", \"rb\") as svdSim_train:\n",
    "    talos_svdsim_train = cPickle.load(svdSim_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.sim.svd.pkl\", \"rb\") as svdSim_test:\n",
    "    talos_svdsim_test = cPickle.load(svdSim_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.headline.word2vec.pkl\", \"rb\") as w2vHealine_train:\n",
    "    talos_w2vHeadline_train = cPickle.load(w2vHealine_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.headline.word2vec.pkl\", \"rb\") as w2vHealine_test:\n",
    "    talos_w2vHeadline_test = cPickle.load(w2vHealine_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.body.word2vec.pkl\", \"rb\") as w2vBody_train:\n",
    "    talos_w2vBody_train = cPickle.load(w2vBody_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.body.word2vec.pkl\", \"rb\") as w2vBody_test:\n",
    "    talos_w2vBody_test = cPickle.load(w2vBody_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.sim.word2vec.pkl\", \"rb\") as w2vSim_train:\n",
    "    talos_w2vsim_train = cPickle.load(w2vSim_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.sim.word2vec.pkl\", \"rb\") as w2vSim_test:\n",
    "    talos_w2vsim_test = cPickle.load(w2vSim_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.headline.senti.pkl\", \"rb\") as sentiHealine_train:\n",
    "    talos_sentiHeadline_train = cPickle.load(sentiHealine_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.headline.senti.pkl\", \"rb\") as sentiHealine_test:\n",
    "    talos_sentiHeadline_test = cPickle.load(sentiHealine_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.body.senti.pkl\", \"rb\") as sentiBody_train:\n",
    "    talos_sentiBody_train = cPickle.load(sentiBody_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.body.senti.pkl\", \"rb\") as sentiBody_test:\n",
    "    talos_sentiBody_test = cPickle.load(sentiBody_test, encoding='latin1')\n",
    "\n",
    "########################## Definir o modelo ##################################### \n",
    "\n",
    "#Define some model layers #\n",
    "print('Done ! Done ! Done !')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "weightedAccuracy = weightedAccCallback(X1_test, X2_test, Y_test, overlapFeatures_fnc_test, refutingFeatures_fnc_test, polarityFeatures_fnc_test, handFeatures_fnc_test,  \\\n",
    "                                       cosFeatures_fnc_test,cosFeatures_two_test, bleu_fnc_test, rouge_fnc_test,cider_fnc_test, X2_test_two_sentences, overlapFeatures_fnc_two_test, \\\n",
    "                                       refutingFeatures_fnc_two_test, polarityFeatures_fnc_two_test, handFeatures_fnc_two_test, \\\n",
    "                                       bleu_two_sentences_test, rouge_two_sentences_test, cider_two_test\n",
    "                                       , talos_counts_test, talos_tfidfsim_test, talos_svdHeadline_test, \\\n",
    "                                       talos_svdBody_test, talos_svdsim_test,talos_w2vHeadline_test, talos_w2vBody_test, talos_w2vsim_test, talos_sentiHeadline_test, talos_sentiBody_test)\n",
    "                                     \n",
    "\n",
    "embedding_layer = Embedding( embedding_weights.shape[0], embedding_weights.shape[1], input_length=max_seq_len, weights=[embedding_weights], trainable=False )\n",
    "lstm1 = LSTM(hidden_units, implementation=2, return_sequences=True, name='lstm1' )\n",
    "lstm1 = Bidirectional(lstm1, name='bilstm1')\n",
    "right_branch_lstm1 = LSTM(hidden_units, implementation=2, return_sequences=True )\n",
    "right_branch_lstm1 = Bidirectional(right_branch_lstm1)\n",
    "\n",
    "#####################################\n",
    "\n",
    "# Define the inputs for the model #\n",
    "\n",
    "input_headline = Input(shape=(max_seq_len,))\n",
    "input_two = Input(shape=(max_seq_len,))\n",
    "input_body = Input(shape=(max_seqs, max_seq_len,))\n",
    "input_overlap = Input(shape=(1,))\n",
    "input_overlap_two = Input(shape=(1,))\n",
    "input_refuting = Input(shape=(15,))\n",
    "input_refuting_two = Input(shape=(15,))\n",
    "input_polarity = Input(shape=(2,))\n",
    "input_polarity_two = Input(shape=(2,))\n",
    "input_hand = Input(shape=(26,))\n",
    "input_hand_two = Input(shape=(26,))\n",
    "input_sim = Input(shape=(1,))\n",
    "input_sim_two = Input(shape=(1,))\n",
    "input_bleu = Input(shape=(1,))\n",
    "input_bleu_two = Input(shape=(1,))\n",
    "input_rouge = Input(shape=(3,))\n",
    "input_rouge_two = Input(shape=(3,))\n",
    "input_cider = Input(shape=(1,))\n",
    "input_cider_two = Input(shape=(1,))\n",
    "\n",
    "input_talos_count = Input(shape=(41,))\n",
    "input_talos_tfidfsim = Input(shape=(1,))\n",
    "input_talos_headline_svd = Input(shape=(50,))\n",
    "input_talos_body_svd = Input(shape=(50,))\n",
    "input_talos_svdsim = Input(shape=(1,))\n",
    "input_talos_headline_w2v = Input(shape=(300,))\n",
    "input_talos_body_w2v = Input(shape=(300,))\n",
    "input_talos_w2vsim = Input(shape=(1,))\n",
    "input_talos_headline_senti = Input(shape=(4,))\n",
    "input_talos_body_senti = Input(shape=(4,))\n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "# Define the sentence encoder #\n",
    "\n",
    "mask = Masking(mask_value=0, input_shape=(max_seq_len,))(input_headline)\n",
    "embed = embedding_layer(mask)\n",
    "l1 = lstm1(embed)\n",
    "drop1 = Dropout(0.1)(l1)\n",
    "maxim = GlobalMaxPooling1D()(drop1)\n",
    "att = SelfAttLayer(name='attention')(drop1)\n",
    "out = concatenate([maxim, att])\n",
    "HeadlineEncoder = Model(input_headline, maxim, name='HeadlineEncoder')\n",
    "\n",
    "# HeadlineEncoder.set_weights(layer_dict['SentenceEncoder'].get_weights())\n",
    "\n",
    "##############################\n",
    "\n",
    "# Define the document encoder #\n",
    "\n",
    "body_sentence = TimeDistributed(HeadlineEncoder)(input_body)\n",
    "body_g1 = right_branch_lstm1(body_sentence)\n",
    "body_g1 = Dropout(0.1)(body_g1)\n",
    "body_maxim = GlobalMaxPooling1D()(body_g1)\n",
    "body_att = SelfAttLayer()(body_g1)\n",
    "body_out = concatenate([body_maxim, body_att])\n",
    "DocumentEncoder = Model(input_body, body_maxim, name='DocumentEncoder')\n",
    "\n",
    "##############################\n",
    "\n",
    "# Combining both representations #\n",
    "\n",
    "headline_representation = HeadlineEncoder(input_headline)\n",
    "document_representation = DocumentEncoder(input_body)\n",
    "\n",
    "# Match between headline and first two sentences from body #\n",
    "\n",
    "two_sentences_representation = HeadlineEncoder(input_two)\n",
    "concat_two = concatenate([headline_representation, two_sentences_representation])\n",
    "mul_two = multiply([headline_representation, two_sentences_representation])\n",
    "dif_two = subtract([headline_representation, two_sentences_representation])\n",
    "final_merge_two = concatenate([concat_two, mul_two, dif_two, input_overlap_two, input_refuting_two, input_polarity_two, input_hand_two, \\\n",
    "                               input_sim_two, input_bleu_two, input_rouge_two, input_cider_two])\n",
    "drop3_two = Dropout(0.1)(final_merge_two)\n",
    "dense1_two = Dense(hidden_units*2, activation='relu')(drop3_two)\n",
    "# , weights=layer_dict['dense1'].get_weights()\n",
    "drop4_two = Dropout(0.1)(dense1_two)\n",
    "dense2_two = Dense(hidden_units, activation='relu')(drop4_two)\n",
    "# ,weights=layer_dict['dense2'].get_weights()\n",
    "match = Dropout(0.1)(dense2_two)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "concat = concatenate([headline_representation, document_representation])\n",
    "mul = multiply([headline_representation, document_representation])\n",
    "dif = subtract([headline_representation, document_representation])\n",
    "final_merge = concatenate([concat, mul, dif, input_overlap, input_refuting, input_polarity, input_hand, input_sim, input_bleu, input_rouge, input_cider])\n",
    "drop3 = Dropout(0.1)(final_merge)\n",
    "dense1 = Dense(hidden_units*2, activation='relu', name='dense1')(drop3)\n",
    "# , weights=layer_dict['dense1'].get_weights()\n",
    "drop4 = Dropout(0.1)(dense1)\n",
    "dense2 = Dense(hidden_units, activation='relu', name='dense2')(drop4)\n",
    "# , weights=layer_dict['dense2'].get_weights()\n",
    "drop5 = Dropout(0.1)(dense2)\n",
    "concat_final = concatenate([drop5,match,input_talos_count, input_talos_tfidfsim, input_talos_headline_svd, input_talos_body_svd, \\\n",
    "                     input_talos_svdsim, input_talos_headline_w2v, input_talos_body_w2v, input_talos_w2vsim, \\\n",
    "                     input_talos_headline_senti, input_talos_body_senti])\n",
    "drop6 = Dropout(0.1)(concat_final)\n",
    "dense3 = Dense(4, activation='softmax')(drop6)\n",
    "final_model = Model([input_headline, input_body,input_overlap, input_refuting, input_polarity, input_hand, \\\n",
    "                     input_sim, input_sim_two, input_bleu, input_rouge,input_cider, input_two, input_overlap_two, input_refuting_two, input_polarity_two, input_hand_two, \\\n",
    "                     input_bleu_two, input_rouge_two, input_cider_two, input_talos_count, input_talos_tfidfsim, input_talos_headline_svd, input_talos_body_svd, \\\n",
    "                     input_talos_svdsim, input_talos_headline_w2v, input_talos_body_w2v, input_talos_w2vsim, \\\n",
    "                     input_talos_headline_senti, input_talos_body_senti], dense3)\n",
    "####################################################################################\n",
    "final_model.summary()\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.load_weights(\"fnc-weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = []\n",
    "test_predictions = []\n",
    "labels = ['unrelated', 'agree', 'disagree', 'discuss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aux = final_model.predict([X1_test, X2_test, overlapFeatures_fnc_test, refutingFeatures_fnc_test, polarityFeatures_fnc_test, handFeatures_fnc_test,  \\\n",
    "                                  cosFeatures_fnc_test, cosFeatures_two_test, bleu_fnc_test, rouge_fnc_test, cider_fnc_test, \\\n",
    "                                  X2_test_two_sentences, overlapFeatures_fnc_two_test, refutingFeatures_fnc_two_test, polarityFeatures_fnc_two_test, handFeatures_fnc_two_test, \\\n",
    "                                  bleu_two_sentences_test, rouge_two_sentences_test, cider_two_test, talos_counts_test, talos_tfidfsim_test, talos_svdHeadline_test, talos_svdBody_test, talos_svdsim_test, \\\n",
    "                                  talos_w2vHeadline_test, talos_w2vBody_test, talos_w2vsim_test, talos_sentiHeadline_test, talos_sentiBody_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "preds = []\n",
    "for prediction in aux:\n",
    "    pred = prediction.argmax()\n",
    "    if pred == 0:\n",
    "        one_hot = [1,0,0,0]\n",
    "    if pred == 1:\n",
    "        one_hot = [0,1,0,0]\n",
    "    if pred == 2:\n",
    "        one_hot = [0,0,1,0]\n",
    "    if pred == 3:\n",
    "        one_hot = [0,0,0,1]\n",
    "    preds += [one_hot]\n",
    "    test_predictions += [labels[prediction.argmax()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unrelated']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
